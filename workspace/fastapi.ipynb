{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing an endpoint in FastAPI\n",
    "\n",
    "In this section, we will create a FastAPI “wrapper” for our model, so that it can serve inference requests. Once you have finished this section, you should be able to:\n",
    "\n",
    "-   create a FastAPI endpoint for a PyTorch model\n",
    "-   create a FastAPI endpoint for an ONNX model\n",
    "\n",
    "and run it on CPU or GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch version\n",
    "\n",
    "We have previously seen a [Flask app](https://github.com/teaching-on-testbeds/gourmetgram/blob/master/app.py) that does inference using a pre-trained PyTorch model, and serves a basic browser-based interface for it.\n",
    "\n",
    "However, to scale up, we will want to separate the model inference service into its own prediction endpoint - that way, we can optimize and scale it separately from the user interface.\n",
    "\n",
    "[Here is the modified version of the Flask app](https://github.com/teaching-on-testbeds/gourmetgram/blob/fastapi/app.py). Instead of loading a model and making predictions, we send a request to a separate service:\n",
    "\n",
    "``` python\n",
    "def request_fastapi(image_path):\n",
    "    try:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "        \n",
    "        encoded_str = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        payload = {\"image\": encoded_str}\n",
    "        \n",
    "        response = requests.post(f\"{FASTAPI_SERVER_URL}/predict\", json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        predicted_class = result.get(\"prediction\")\n",
    "        probability = result.get(\"probability\")\n",
    "        \n",
    "        return predicted_class, probability\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")  \n",
    "        return None, None  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the inference service has moved into a separate app:\n",
    "\n",
    "``` python\n",
    "app = FastAPI(\n",
    "    title=\"Food Classification API\",\n",
    "    description=\"API for classifying food items from images\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "# Define the request and response models\n",
    "class ImageRequest(BaseModel):\n",
    "    image: str  # Base64 encoded image\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    probability: float = Field(..., ge=0, le=1)  # Ensures probability is between 0 and 1\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the Food11 model\n",
    "MODEL_PATH = \"food11.pth\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define class labels\n",
    "classes = np.array([\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n",
    "    \"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\", \"Vegetable/Fruit\"])\n",
    "\n",
    "# Define the image preprocessing function\n",
    "def preprocess_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict_image(request: ImageRequest):\n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_data = base64.b64decode(request.image)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess the image\n",
    "        image = preprocess_image(image).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            probabilities = F.softmax(output, dim=1)  # Apply softmax to get probabilities\n",
    "            predicted_class = torch.argmax(probabilities, 1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()  # Get the probability\n",
    "\n",
    "        return PredictionResponse(prediction=classes[predicted_class], probability=confidence)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "```\n",
    "\n",
    "Let’s try it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring up containers\n",
    "\n",
    "To start, run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-fastapi.yaml up -d\n",
    "```\n",
    "\n",
    "This will bring up three containers:\n",
    "\n",
    "-   one container that will host the Flask application, this will serve the web-based user interface of our system\n",
    "-   one container that will host a FastAPI inference endpoint\n",
    "-   and one Jupyter container, which we’ll use to run some benchmarking experiments\n",
    "\n",
    "Check the logs of the Jupyter container:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs jupyter\n",
    "```\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of 127.0.0.1, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface that is running *on your compute instance*.\n",
    "\n",
    "Then, in the file browser on the left side, open the “work” directory and then click on the `fastapi.ipynb` notebook to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test this service. First, we’ll test the FastAPI endpoint directly. In a browser, run\n",
    "\n",
    "    http://A.B.C.D:8000/docs\n",
    "\n",
    "but substitute the floating IP assigned to your instance. This will bring up the [Swagger UI](https://swagger.io/tools/swagger-ui/) associated with the FastAPI endpoint.\n",
    "\n",
    "Click on “predict” and then “Try it out”. Here, we can enter a request to send to the FastAPI endpoint, and see its response.\n",
    "\n",
    "Our request needs to be in the form of a base64-encoded image. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "image_path = \"test_image.jpeg\"\n",
    "with open(image_path, 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "encoded_str =  base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "print('\"' + encoded_str + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the encoded image string. Copy the output of that cell. (After you copy it, you can right-click and clear the cell output, so it won’t clutter up the notebook interface.)\n",
    "\n",
    "Then, in\n",
    "\n",
    "    {\n",
    "      \"image\": \"string\"\n",
    "    }\n",
    "\n",
    "replace “string” with the encoded image string you just copied. Press “Execute”.\n",
    "\n",
    "You should see that the server returns a response with code 200 (that’s the response code for a successful request) and a response body like:\n",
    "\n",
    "    {\n",
    "      \"prediction\": \"Vegetable/Fruit\",\n",
    "      \"probability\": 0.9940803647041321\n",
    "    }\n",
    "\n",
    "so we can see that it performed inference successfully on the test input.\n",
    "\n",
    "Next, let’s check the integration of the FastAPI endpoint in our Flask app. In your browser, open\n",
    "\n",
    "    http://A.B.C.D\n",
    "\n",
    "but substitute the floating IP assigned to your instance, to access the Flask app. Upload an image and press “Submit” to get its class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know everything *works*, let’s get some quick performance numbers from this server. We’ll send some requests directly to the FastAPI endpoint and measure the time to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTAPI_URL = \"http://fastapi_server:8000/predict\"\n",
    "payload = {\"image\": encoded_str}\n",
    "num_requests = 100\n",
    "inference_times = []\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(FASTAPI_URL, json=payload)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        inference_times.append(end_time - start_time)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_times = np.array(inference_times)\n",
    "median_time = np.median(inference_times)\n",
    "percentile_95 = np.percentile(inference_times, 95)\n",
    "percentile_99 = np.percentile(inference_times, 99)\n",
    "throughput = num_requests / inference_times.sum()  \n",
    "\n",
    "print(f\"Median inference time: {1000*median_time:.4f} ms\")\n",
    "print(f\"95th percentile: {1000*percentile_95:.4f} ms\")\n",
    "print(f\"99th percentile: {1000*percentile_99:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} requests/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "Median inference time: 17.2018 ms\n",
    "95th percentile: 19.4870 ms\n",
    "99th percentile: 22.2096 seconds\n",
    "Throughput: 57.16 requests/sec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX version\n",
    "\n",
    "We know from our previous experiments that the vanilla PyTorch model may not be optimized for inference speed.\n",
    "\n",
    "Let’s try porting our FastAPI endpoint to ONNX.\n",
    "\n",
    "On the “node-serve-system” host, edit the Docker compose file:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/serve-system-chi/docker/docker-compose-fastapi.yaml\n",
    "```\n",
    "\n",
    "and modify\n",
    "\n",
    "          context: /home/cc/serve-system-chi/fastapi_pt\n",
    "\n",
    "to\n",
    "\n",
    "          context: /home/cc/serve-system-chi/fastapi_onnx\n",
    "\n",
    "to build the FastAPI container image from the “fastapi_onnx” directory, instead of the “fastapi_pt” directory.\n",
    "\n",
    "Save your changes (Ctrl+O, Enter, Ctrl+X). Rebuild the container image:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-fastapi.yaml build fastapi_server\n",
    "```\n",
    "\n",
    "and recreate the container with the new image:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-fastapi.yaml up fastapi_server --force-recreate -d\n",
    "```\n",
    "\n",
    "Repeat the same steps as before to test the FastAPI endpoint and its integration with Flask.\n",
    "\n",
    "Then, re-do our quick benchmark.\n",
    "\n",
    "(This host is running an older GPU, so we won’t attempt to use the TensorRT execution provider for ONNX, because modern versions no longer support it. So, our results won’t be *too* dramatic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTAPI_URL = \"http://fastapi_server:8000/predict\"\n",
    "payload = {\"image\": encoded_str}\n",
    "num_requests = 100\n",
    "inference_times = []\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(FASTAPI_URL, json=payload)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        inference_times.append(end_time - start_time)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_times = np.array(inference_times)\n",
    "median_time = np.median(inference_times)\n",
    "percentile_95 = np.percentile(inference_times, 95)\n",
    "percentile_99 = np.percentile(inference_times, 99)\n",
    "throughput = num_requests / inference_times.sum()  \n",
    "\n",
    "print(f\"Median inference time: {1000*median_time:.4f} ms\")\n",
    "print(f\"95th percentile: {1000*percentile_95:.4f} ms\")\n",
    "print(f\"99th percentile: {1000*percentile_99:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} requests/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "Median inference time: 9.2471 ms\n",
    "95th percentile: 11.2387 ms\n",
    "99th percentile: 16.1481 seconds\n",
    "Throughput: 80.66 requests/sec\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our FastAPI endpoint can maintain low latency, as long as only one user is sending requests to the service.\n",
    "\n",
    "However, when there are multiple concurrent requests, it will be much slower. For example, suppose we start 16 “senders” at the same time, each continuously sending a new request as soon as it gets a response for the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def send_request(payload):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(FASTAPI_URL, json=payload)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return end_time - start_time\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, Response: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def run_concurrent_tests(num_requests, payload, max_workers=10):\n",
    "    inference_times = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(send_request, payload) for _ in range(num_requests)]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                inference_times.append(result)\n",
    "    \n",
    "    return inference_times\n",
    "\n",
    "num_requests = 1000\n",
    "start_time = time.time()\n",
    "inference_times = run_concurrent_tests(num_requests, payload, max_workers=16)\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_times = np.array(inference_times)\n",
    "median_time = np.median(inference_times)\n",
    "percentile_95 = np.percentile(inference_times, 95)\n",
    "percentile_99 = np.percentile(inference_times, 99)\n",
    "throughput = num_requests / total_time\n",
    "\n",
    "print(f\"Median inference time: {1000*median_time:.4f} ms\")\n",
    "print(f\"95th percentile: {1000*percentile_95:.4f} ms\")\n",
    "print(f\"99th percentile: {1000*percentile_99:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} requests/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a request arrives at the server and finds it busy processing another request, it waits in a queue until it can be served. This queuing delay can be a significant part of the overall prediction delay, when there is a high degree of concurrency. We will attempt to address this in the next section!\n",
    "\n",
    "In the meantime, bring down your current inference service with:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-fastapi.yaml down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, download this entire notebook for later reference."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
